{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3752e76c",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Visualization of Climate Data\n",
    "Let's start our big data viz journey by using some sample climate datasets in GeoTIFF format, downloaded from the [PRISM online repository](https://prism.oregonstate.edu/downloads/) by Oregon State University. If you are interested in how to programmatically download these data, please refer to [this blog post](https://medium.com/@mahyar.aboutalebi/how-to-download-process-and-visualize-climate-data-in-python-cfd6d8350322). To simplify things, we pre-downloaded the data for this training. In this step, we will read a raster file of the mean air temperature for the CONUS extent for July of 2025, replace -9999 values with NaN, create a mesh grid for latitude and longitude, and set up the titles, x-axis and y-axis labels, and color ramp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04298e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#s3_url = 's3://your-bucket-name/path/to/your/image.tif'\n",
    "s3_url = 's3://sagemaker-us-west-2-482277551691/ftonini/prism_tmean_us_25m_2000_2025_07/prism_tmean_us_25m_202507.tif'\n",
    "\n",
    "# Open the raster file\n",
    "with rasterio.open(f'{s3_url}') as src:\n",
    "    # Read the temperature data from the first band\n",
    "    temp = src.read(1)\n",
    "    # Replace -9999 values with NaN\n",
    "    temp[temp == -9999] = np.nan\n",
    "    # Get the latitude and longitude coordinates\n",
    "    lon_min, lat_min, lon_max, lat_max = src.bounds\n",
    "    lat_res, lon_res = src.res\n",
    "    lats = np.arange(lat_min, lat_max, lat_res)\n",
    "    lons = np.arange(lon_min, lon_max, lon_res)\n",
    "    lons, lats = np.meshgrid(lons, lats)\n",
    "\n",
    "    # Set the minimum and maximum values of the color scale\n",
    "    vmin = 10\n",
    "    vmax = 30\n",
    "\n",
    "    # Plot the temperature data with a color ramp\n",
    "    cmap = plt.colormaps.get_cmap('jet')\n",
    "    cmap.set_bad('white')\n",
    "\n",
    "    # Set the size of the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    # Plot the temperature data with a color ramp\n",
    "    im = ax.imshow(temp, cmap=cmap, vmin=vmin, vmax=vmax, extent=[lons.min(), lons.max(), lats.min(), lats.max()])\n",
    "\n",
    "    # Add a colorbar with a smaller size\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.5)\n",
    "\n",
    "    # Set the x and y axis labels\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "\n",
    "    # Set the title of the plot\n",
    "    ax.set_title('Temperature for July')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df825f",
   "metadata": {},
   "source": [
    "## üìÇ Data Cube with Random Numbers\n",
    "Let's take a step forward and move to multidimensional data. To create a cubic dataset with random numbers, we will import numpy, xarray, zarr, and pandas libraries. We will generate 720 latitudes from -90 to +90, 1440 longitudes from -180 to +180, and 365 days from 2023‚Äì01‚Äì01 to 2023‚Äì12‚Äì31. Next, we will create a 3D array with random numbers ranging from -20 to 30 and set time, latitude, and longitude. After that, we will convert the array to Xarray, save it in Zarr format, and label the data as \"air temperature\". The following lines do these steps for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb96b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import pandas as pd\n",
    "\n",
    "# Set dimensions\n",
    "lat = np.linspace(-90, 90, 720)\n",
    "lon = np.linspace(-180, 180, 1440)\n",
    "time = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "\n",
    "# Create 3D array with random numbers\n",
    "data = np.random.uniform(low=-20, high=30, size=(len(time), len(lat), len(lon)))\n",
    "\n",
    "# Create xarray DataArray with coordinate labels\n",
    "data_array = xr.DataArray(data, coords={\"time\": time, \"lat\": lat, \"lon\": lon}, dims=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "# Save DataArray as Zarr file\n",
    "data_array.to_dataset(name=\"air_temperature\").to_zarr(\"random_data.zarr\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02c87d",
   "metadata": {},
   "source": [
    "You can open this data cube with these lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaff90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"random_data.zarr\", chunks={}, engine=\"zarr\")\n",
    "da = ds[\"air_temperature\"][:,:,:]\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedc9aa",
   "metadata": {},
   "source": [
    "If the data cube is created correctly, you will see the variable name (in this case, air temperature) and dimensions for time (365), latitude (720), and longitude (1440). In the next section, we will use this data cube to plot it with Lexcube. But before that, let‚Äôs create a data cube with the climate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc65592",
   "metadata": {},
   "source": [
    "## üìÇ Get a list of GeoTIFF files from an S3 folder \n",
    "To get a list of all .tif files inside a specific S3 \"folder\" (prefix), you need to use the boto3 library to list all objects with the specified prefix and then filter the results client-side to keep only those ending with .tif. S3 does not support filtering by suffix on the service end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def list_tif_files_in_s3_folder(bucket_name, folder_prefix):\n",
    "    \"\"\"\n",
    "    Lists all .tif files within a specific folder prefix in an S3 bucket.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :param folder_prefix: The 'folder' path (prefix) within the bucket.\n",
    "    :return: A list of S3 object keys (file names) ending with '.tif'.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    tif_files = []\n",
    "\n",
    "    # Use a paginator to handle cases with more than 1000 objects\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name, Prefix=folder_prefix)\n",
    "\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                # Client-side filtering for the .tif suffix\n",
    "                if obj['Key'].lower().endswith('.tif') or obj['Key'].lower().endswith('.tiff'):\n",
    "                    tif_files.append(obj['Key'])\n",
    "                    \n",
    "    return tif_files\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Replace with your bucket name and folder prefix\n",
    "s3_bucket_name = 'sagemaker-us-west-2-482277551691'\n",
    "s3_folder_prefix = 'ftonini/prism_tmean_us_25m_2000_2025_07/' # Ensure a trailing slash for a specific folder\n",
    "\n",
    "# Get the list of .tif files\n",
    "geotiff_files = list_tif_files_in_s3_folder(s3_bucket_name, s3_folder_prefix)\n",
    "\n",
    "# Print the list of files\n",
    "if geotiff_files:\n",
    "    print(f\"Found {len(geotiff_files)} .tif files in s3://{s3_bucket_name}/{s3_folder_prefix}:\")\n",
    "    for file_key in geotiff_files:\n",
    "        print(file_key)\n",
    "else:\n",
    "    print(f\"No .tif files found in s3://{s3_bucket_name}/{s3_folder_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd209ab",
   "metadata": {},
   "source": [
    "## üåç Data Cube with Climate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3a856",
   "metadata": {},
   "source": [
    "To create the data cube with climate data, we will use the GeoTIFF files from the PRISM climate datasets pre-downloaded for this training. You should have the mean air temperature of the US for July from 2000 to 2025 in GeoTIFF format in your content folder. Before reading the raster files in a loop, we need to create a list of date ranges for those GeoTIFF files, as the GeoTIFF files cover July from 2000 to 2025. Next, we will read each raster file using the rasterio package, convert the raster file to Xarray with latitude and longitude extracted from each file, and create a data cube by appending the 2D data from each raster file. Finally, we‚Äôll set the time in the data cube based on the time range we generated and save it as a Zarr file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ab288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import xarray as xr\n",
    "import zarr\n",
    "\n",
    "# Create a time list\n",
    "time = pd.date_range(start='2000-07-01', end='2025-07-01', freq='YS-JUL')\n",
    "\n",
    "# Read raster files and create a list of xarray DataArrays\n",
    "data_arrays = []\n",
    "for file in geotiff_files:\n",
    "    with rasterio.open(os.path.join('s3://', s3_bucket_name, file)) as src:\n",
    "        data = src.read(1)  # Read the first band\n",
    "        height, width = src.shape\n",
    "        y_values = np.arange(height) * src.transform[4] + src.transform[5]\n",
    "        x_values = np.arange(width) * src.transform[0] + src.transform[2]\n",
    "        da = xr.DataArray(data, dims=(\"y\", \"x\"), coords={\"y\": y_values, \"x\": x_values}, name=\"air_temperature\")\n",
    "        data_arrays.append(da)\n",
    "\n",
    "# Combine DataArrays into a single xarray Dataset\n",
    "ds = xr.concat(data_arrays, dim=\"time\")\n",
    "ds[\"time\"] = (\"time\", time)  # Assign month numbers as time coordinates\n",
    "\n",
    "# Save the xarray Dataset as a zarr file\n",
    "ds.to_zarr(\"climate.zarr\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1e2c8",
   "metadata": {},
   "source": [
    "Similar to the previous section, we can read the saved Zarr file using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"climate.zarr\", chunks={}, engine=\"zarr\")\n",
    "da = ds[\"air_temperature\"][:,:,:]\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20464cb",
   "metadata": {},
   "source": [
    "If the file is saved correctly, you should see the Xarray details (26 layers in time, 1405 grids on the x-axis, and 621 grids on the y-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d4351",
   "metadata": {},
   "source": [
    "## üåç 3D Visualization of Xarray by Lexcube\n",
    "Let's now use the [lexcube package](https://github.com/msoechting/lexcube) to create some nifty 3D visualizations. Now that we have two Zarr files (one based on random numbers and the second based on climate data), we are ready to plot the 3D visualization of these two data cubes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec05ba5",
   "metadata": {},
   "source": [
    "Let‚Äôs start with the first data cube (random numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import lexcube\n",
    "ds = xr.open_dataset(\"random_data.zarr\", chunks={}, engine=\"zarr\")\n",
    "da = ds[\"air_temperature\"][:,:,:]\n",
    "w = lexcube.Cube3DWidget(da,cmap=\"thermal_r\", vmin=-20, vmax=30)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a86b80",
   "metadata": {},
   "source": [
    "Since the first data cube has been generated based on random numbers, you will see a completely noisy image in all three dimensions. However, it would be a good exercise to demonstrate how to convert any data cube to Xarray for use in Lexcube. To have a better and more meaningful 3D plot, let‚Äôs visualize the second data cube (climate data). We will replicate the same lines but with the new Zarr file name, which is ‚Äòclimate.zarr‚Äô:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcaf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"climate.zarr\", chunks={}, engine=\"zarr\")\n",
    "da = ds[\"air_temperature\"][:,:,:]\n",
    "\n",
    "w = lexcube.Cube3DWidget(da,cmap=\"RdYlBu_r\", vmin=0, vmax=30)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d4b5b",
   "metadata": {},
   "source": [
    "Let‚Äôs zoom in on Central Valley in California. You can see the mean air temperature for July 2020 displayed on the top layer. Additionally, by hovering your mouse over the latitude and longitude axes, you can see the air temperature for different years. Which year was the warmest? At which coordinates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb143ca8",
   "metadata": {},
   "source": [
    "## üì¶ What Else Can We Do with Lexcube\n",
    "Let‚Äôs assume you want to clip this plot for a specific location (latitude and longitude) and a specific time. Instead of manually zooming in and out on each axis, which is not very convenient, you can activate the slider by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a22a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.show_sliders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092631fc",
   "metadata": {},
   "source": [
    "with that slider, you can clip your plot for any location and timeframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ac12d",
   "metadata": {},
   "source": [
    "And last but not least, if you want to save the plot in your local folder, you can do so by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.savefig(fname=\"climate.png\", include_ui=True, dpi_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8270c44",
   "metadata": {},
   "source": [
    "The PNG file will be saved in your folder. If you want to change the color ramp, Lexcube supports many color maps that you can find in the GitHub repository mentioned in the reference section. Also, if you want to plot other geospatial datasets with Lexcube, check out the following post for downloading satellite and geospatial databases for a specific location and times without writing any code:\n",
    "\n",
    "[How to Download Satellite Images Without Writing a Single Line of Code](https://medium.com/@mahyar.aboutalebi/how-to-download-satellite-images-without-writing-a-single-line-of-code-c08b1f910203)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d78017f",
   "metadata": {},
   "source": [
    "## üìù Conclusion\n",
    "Data visualization nowadays is a great skill that can help us tell stories and efficiently present the outcomes in our reports. Lexcube is one of the libraries in this field that helps us visualize data cubes, particularly 3D geospatial databases, quickly and in a visually engaging way. In this post, we cover how to prepare your datasets for this library and how to use it for your data visualization projects. Specifically, we created two datasets: one based on completely random numbers and the second based on air temperature reported by climate databases. Additional sources are provided if you want to create your data cube for any location and time frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad83eb",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "M., S√∂chting, M. D., Mahecha, D., Montero, and G., Scheuermann, Lexcube: Interactive Visualization of Large Earth System Data Cubes (2023). IEEE Computer Graphics and Applications. doi:10.1109/MCG.2023.3321989.\n",
    "\n",
    "[https://github.com/msoechting/lexcube](https://github.com/msoechting/lexcube)\n",
    "\n",
    "[https://pubmed.ncbi.nlm.nih.gov/37812545/](https://pubmed.ncbi.nlm.nih.gov/37812545/)\n",
    "\n",
    "[https://eo4society.esa.int/2022/05/25/exploring-earth-system-data-with-lexcube/](https://eo4society.esa.int/2022/05/25/exploring-earth-system-data-with-lexcube/)\n",
    "\n",
    "[https://www.linkedin.com/posts/miguel-mahecha-625548197_lexcube-activity-7156200204700966913-5lOb?utm_source=share&utm_medium=member_desktop](https://www.linkedin.com/posts/miguel-mahecha-625548197_lexcube-activity-7156200204700966913-5lOb?utm_source=share&utm_medium=member_desktop)\n",
    "\n",
    "PRISM Climate Group, Oregon State University, [https://prism.oregonstate.edu](https://prism.oregonstate.edu), date created 1981‚Äì2022, accessed 19 Dec 2024."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
